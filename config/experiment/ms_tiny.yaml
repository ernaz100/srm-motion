# @package _global_

defaults:
  - override /dataset: mnist_sudoku_tiny
  - override /model/denoiser: unet_128  # Smaller model to save memory

conditioning:
  mask: true

validation:
  sampling:
    name: sampling  # Use simple sampling instead of mnist_sudoku
    num_samples: 2  # Only validate on 2 samples for tiny dataset

wandb:
  tags: [sudoku, mnist, tiny]
  project: srm
  entity: ernaz100
  mode: online
  activated: true
  
data_loader:
  train:
    # Reduced batch size for tiny dataset
    batch_size: 2  # Reduced from 8
    num_workers: 0  # Disable multiprocessing to avoid pickling errors
    persistent_workers: false
  val:
    batch_size: 2  # Reduced from 8
    num_workers: 0  # Disable multiprocessing
    persistent_workers: false
  test:
    batch_size: 2  # Reduced from 8
    num_workers: 0  # Disable multiprocessing
    persistent_workers: false

# Memory optimization settings
trainer:
  max_steps: 10_000  # Reduced from 250_001
  val_check_interval: 500  # More frequent validation
  log_every_n_steps: 1    # More frequent logging
  precision: bf16-mixed  # Use mixed precision to save memory
  accumulate_grad_batches: 4  # Effective batch size = 2 * 4 = 8

# PyTorch memory settings
torch:
  cudnn_benchmark: false  # Disable to save memory
  float32_matmul_precision: medium  # Reduce precision to save memory 